{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cybersecurity-Syscall_Mimicry_using_LSTM.py",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM74PFTj9MhYcnhooI33DhD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yumakemore/Cybersecurity-Syscall_Mimicry_Using_LSTM/blob/master/Cybersecurity_Syscall_Mimicry_using_LSTM_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b73FtHy3PvDr",
        "colab_type": "text"
      },
      "source": [
        "#Cybersecurity-Syscall_Mimicry_Using_LSTM\n",
        "\n",
        "Byunggu Yu, Ph.D.\n",
        "\n",
        "May 2017\n",
        "\n",
        "Deep LSTM Long Short Term Neural Network compiling a syscall mimicry.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1QuE4x2X7iu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import re\t# Regix\n",
        "import sys\t\n",
        "from scipy.special import expit as sigmoid\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "#============= Global Constants and Variables ================\n",
        "L=2\t\t\t# number of hidden layers\n",
        "hlayer_size = 256 \t# number of neurons in a hidden layer\n",
        "num_steps = 25 \t\t# BPTT step size\n",
        "learning_rate = 0.01\n",
        "init_scale = 0.1\t# initial W is random uniform [-init_scale, init_scale]\n",
        "\n",
        "data=None\n",
        "vocab_size=None\n",
        "mode=None\n",
        "word_to_idx=None\n",
        "idx_to_word=None\n",
        "\n",
        "wL=None\n",
        "bL=None\n",
        "\n",
        "W=[None]*L\n",
        "H=[None]*L\n",
        "B=[None]*L\n",
        "\n",
        "last_c=[None]*L\n",
        "last_h=[None]*L\n",
        "\n",
        "#=============================================================\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLu9ThmnpR82",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def setup(infile):\n",
        "\n",
        "\tglobal data, vocab_size, mode, word_to_idx, idx_to_word\n",
        "\tglobal W, wL\n",
        "\tglobal B, bL\n",
        "\tglobal last_h, last_c \n",
        "\n",
        "\t#======= Read in data\n",
        "\ttry:\n",
        "\t\tdata = open(infile, 'r').read().strip() \n",
        "\texcept:\n",
        "\t\tprint (\"Failed to read the inpur file\\n\")\n",
        "\t\texit(0)\n",
        "\n",
        "\n",
        "\t#======== Choose either word-level processing or char-level processing\n",
        "\n",
        "\t#--- Use the following for the char-level processing\n",
        "\t#mode=\"c\"\n",
        "\t#words = list(set(data))\n",
        "\t#---------------------------------------------------\n",
        "\n",
        "\t#--- Use the following for the word-level processing\n",
        "\tmode=\"w\"\n",
        "\t# Optional text pre-processor (customizable and removable)\n",
        "\tif mode==\"w\":\n",
        "\t\tdata=re.sub('([\\\\a\\\\b\\\\f\\\\n\\\\r\\\\t\\\\v])', ' ', data)\n",
        "\t\tdata=re.sub('\\s{2,}', ' ', data)\n",
        "\tdata=list(data.split(\" \"))\n",
        "\twords = list(set(data))\n",
        "\t#---------------------------------------------------\n",
        "\n",
        "\n",
        "\ttry:\n",
        "\t\twords.remove(\"\")\n",
        "\texcept:\n",
        "\t\tpass\n",
        "\n",
        "\tdata_size, vocab_size = len(data), len(words)\n",
        "\tprint('data has %d words, %d unique.' % (data_size, vocab_size))\n",
        "\tinput(\"press a key to start:\\n\")\n",
        "\tword_to_idx = { word:idx for idx,word in enumerate(words) }\n",
        "\tidx_to_word = { idx:word for idx,word in enumerate(words) }\n",
        "\n",
        "\t#======= w, u, and b of each hidden layer\n",
        "\tW[0] = np.random.uniform(low=-init_scale, high=init_scale, size=(hlayer_size*4, vocab_size+hlayer_size))\n",
        "\tB[0] = np.zeros((hlayer_size*4, 1))\n",
        "\tfor k in range(1,L): # for each hidden layer k,\n",
        "\t\tW[k] = np.random.uniform(low=-init_scale, high=init_scale, size=(hlayer_size*4, hlayer_size*2))\n",
        "\t\tB[k] = np.zeros((hlayer_size*4, 1))\n",
        "\t\tlast_c[k] = np.zeros((hlayer_size, 1))\n",
        "\t\tlast_h[k] = np.zeros((hlayer_size, 1))\n",
        "\t#======= w and b of the readout layer\n",
        "\twL = np.random.uniform(low=-init_scale, high=init_scale, size=(vocab_size, hlayer_size)) \n",
        "\tbL = np.zeros((vocab_size, 1)) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oF35iuP8pps7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ForwardBackward(inputs, targets):\n",
        "  global last_c, last_h \n",
        "  x, y = {}, {}\n",
        "  h, f, i, a, o, dh = {}, {}, {}, {}, {}, {}\n",
        "  c = {}\n",
        " \n",
        "  h[-1] = np.copy(last_h)\n",
        "  \n",
        "  sumOfallC = 0\n",
        "\n",
        "\n",
        "  #=========== Feedforward Through Time (FFTT-SRN)\n",
        "\n",
        "  #--- Memory Preparation\n",
        "  for t in range(0, len(inputs)):\n",
        "    h[t]=[None]*(L+1) # [-1] to store x\n",
        "    f[t]=[None]*L\n",
        "    i[t]=[None]*L\n",
        "    a[t]=[None]*L\n",
        "    o[t]=[None]*L\n",
        "    c[t]=[None]*L\n",
        "\n",
        "  c[-1]=np.copy(last_c)\n",
        "  #---\n",
        "\n",
        "  for t in range(0,len(inputs)): # t is reset of zero for truncated BPTT\n",
        "    x[t] = np.zeros((vocab_size,1)) \n",
        "    x[t][inputs[t]] = 1 # one-hot representation\n",
        "\n",
        "    #--- Hidden Layers ---\n",
        "    h[t][-1]=x[t]\n",
        "    for l in range(0,L):\n",
        "      H = np.concatenate((h[t][l-1],h[t-1][l]),axis=0)\n",
        "      z=np.dot(W[l], H) + B[l]\n",
        "\n",
        "      unit=int(z.shape[0]/4)\n",
        "      f[t][l]=sigmoid(z[0:unit])\n",
        "      i[t][l]=sigmoid(z[unit:2*unit])\n",
        "      a[t][l]=np.tanh(z[2*unit:3*unit])\n",
        "      o[t][l]=sigmoid(z[3*unit:4*unit])\n",
        "\n",
        "      c[t][l]=f[t][l]*c[t-1][l]+i[t][l]*a[t][l] \n",
        "      h[t][l]=o[t][l]*np.tanh(c[t][l]) \n",
        "\n",
        "    #--- Readout Layer ---\n",
        "    zL=np.dot(wL, h[t][L-1]) + bL\n",
        "    y[t] = zL # logit\n",
        "    y[t] = np.exp(y[t]) / np.sum(np.exp(y[t])) # omaga: softmax normalization\n",
        "    sumOfallC += -np.log(y[t][targets[t],0]) # loss in cross entropy for each target in one-hot representation\n",
        "\n",
        "  #--- store the last c and h for the next step sequence\n",
        "  last_c=np.copy(c[len(inputs)-1])\n",
        "  last_h=np.copy(h[len(inputs)-1])\n",
        "\n",
        "\n",
        "  #=========== Truncated Backpropagation Through Time (Truncated BPTT-SRN)\n",
        "  #--- Memory Preparation\n",
        "  for t in range(0, len(inputs)):\n",
        "    dh[t]=[None]*L\n",
        "  #-----------------------\n",
        "  #---- initialize db, dw, du ----\n",
        "  dwL = np.zeros_like(wL) \n",
        "  dbL = np.zeros_like(bL) \n",
        "  dW=[None]*L\n",
        "  dB=[None]*L\n",
        "  for k in range(0,L): # for each hidden layer k,\n",
        "\t  dW[k] = np.zeros_like(W[k])\n",
        "\t  dB[k] = np.zeros_like(B[k])\n",
        "  #--------------------------------\n",
        "\n",
        "  for t in reversed(range(0,len(inputs))):\n",
        "    dz=np.copy(y[t])\n",
        "    dz[targets[t]] -= 1 # (y - target) : for the one-hot representation case\n",
        "    \n",
        "    dbL += dz\n",
        "\n",
        "    dwL += np.dot(dz, h[t][L-1].T)\n",
        "\n",
        "    if t==len(inputs)-1:\n",
        "      dh[t][L-1]=np.dot(wL.T, dz)\n",
        "\n",
        "    for l in reversed(range(0,L)):\n",
        "      do=dh[t][l]*np.tanh(c[t][l])\n",
        "      dc=dh[t][l]*(1-np.tanh(c[t][l])*np.tanh(c[t][l]))\n",
        "      da=dc*i[t][l]\n",
        "      di=dc*a[t][l]\n",
        "      df=dc*c[t-1][l]\n",
        "\n",
        "      dzf=df*f[t][l]*(1-f[t][l])\n",
        "      dzi=di*i[t][l]*(1-i[t][l])\n",
        "      dza=da*(1-a[t][l]*a[t][l]) \n",
        "      dzo=do*o[t][l]*(1-o[t][l])\n",
        "\n",
        "      dz=np.concatenate((dzf, dzi, dza, dzo), axis=0)\n",
        "      H = np.concatenate((h[t][l-1],h[t-1][l]),axis=0)\n",
        "      dB[l]+=dz\n",
        "      dW[l]+=np.dot(dz,H.T)\n",
        "      dH=np.dot(W[l].T,dz)\n",
        "      if l>0:\n",
        "        dh[t][l-1]=dH[0:int(dH.shape[0]/2)]\n",
        "      if t>0:\n",
        "        dh[t-1][l]=dH[int(dH.shape[0]/2):dH.shape[0]]\n",
        "\n",
        "  #--- Optional Clipping\n",
        "  for dparam in [dwL, dbL]:\n",
        "    np.clip(dparam, -5, 5, out=dparam) \n",
        "  for dparam in dW:\n",
        "    np.clip(dparam, -5, 5, out=dparam) \n",
        "  for dparam in dB:\n",
        "    np.clip(dparam, -5, 5, out=dparam) \n",
        "  #---------------------\n",
        "\n",
        "  return sumOfallC, dwL, dbL, dW, dB"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CUxROVzq1kc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def creation(beginning_idx, n):\n",
        "  \"\"\" \n",
        "  Auto-generation/creation mode\n",
        "  \"\"\"\n",
        "  c=np.copy(last_c)\n",
        "  h=np.copy(last_h)\n",
        "\n",
        "  h_l_minus_1 = np.zeros((vocab_size,1))\n",
        "  h_l_minus_1[beginning_idx] = 1\n",
        "\n",
        "  idxes = []\n",
        "  idxes.append(beginning_idx)\n",
        "  for t in range(0,n): \n",
        "    x_ravel=np.copy(h_l_minus_1)\n",
        "    for l in range(0,L):\n",
        "      H = np.concatenate((h_l_minus_1,h[l]),axis=0)\n",
        "      z=np.dot(W[l], H) + B[l]\n",
        "\n",
        "      unit=int(z.shape[0]/4)\n",
        "      f=sigmoid(z[0:unit])\n",
        "      i=sigmoid(z[unit:2*unit])\n",
        "      a=np.tanh(z[2*unit:3*unit])\n",
        "      o=sigmoid(z[3*unit:4*unit])\n",
        "      c[l]+=f*c[l]+i*a \n",
        "\n",
        "      h[l]=o*np.tanh(c[l]) \n",
        "      h_l_minus_1=np.copy(h[l])\n",
        "\n",
        "    z = np.dot(wL, h[L-1]) + bL\n",
        "    y = np.exp(z) / np.sum(np.exp(z))\n",
        "\t\n",
        "    #input(\"Press a key\")\n",
        "    y_ravel=y.ravel()\n",
        "    if t==0:\n",
        "    \tidx = np.argmax(y_ravel)\n",
        "    else:\n",
        "      idx = np.random.choice(range(vocab_size), p=y.ravel())\n",
        "      #idx = np.argmax(y_ravel)\n",
        "    '''\n",
        "    if x_ravel[idx] == 1: #do not output the input (no repeating)\n",
        "      y_ravel[idx]=0\n",
        "      idx=np.argmax(y_ravel)\n",
        "    '''\n",
        "    h_l_minus_1 = np.zeros((vocab_size, 1))\n",
        "    h_l_minus_1[idx] = 1\n",
        "    idxes.append(idx)\n",
        "\n",
        "  '''\n",
        "  This creation is outside the sequence of the given data and \n",
        "  to be forgotten. Therefore last_c and last_h are not updated.\n",
        "  '''\n",
        "  return idxes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ll1UAeMaaEL-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#==========================================================================#\n",
        "if __name__ == \"__main__\":\n",
        "#==========================================================================#\n",
        "\n",
        "  #=== Pyplot variables\n",
        "  plt.figure(\"Syscall Test\")\n",
        "  #plt.ion()\n",
        "  plot_x=[]\n",
        "  plot_y1=[]\n",
        "  plot_y2=[]\n",
        "  #====================\n",
        "  \n",
        "  '''\n",
        "  if len(sys.argv) != 2:\n",
        "    print(\"Usage: simple-rnn.py <input file>\\n\")\n",
        "    exit(0)\n",
        "  \n",
        "  setup(str(sys.argv[1]))\n",
        "  '''\n",
        "  \n",
        "  print(\"This program properly runs on Python 3.x\\n\")\n",
        "  input_f = input(\"Give a data file in a plain text file (e.g., ./test-fixed.txt): \\n\")\n",
        "  setup(input_f)\n",
        "  \n",
        "  num_of_steps = 0\n",
        "  data_read_point = 0\n",
        "  #---- initialize mw, mu, and mb for Adagrad ----\n",
        "  mwL=None\n",
        "  mbL=None\n",
        "  mW=[None]*L\n",
        "  mB=[None]*L\n",
        "\n",
        "  mwL = np.zeros_like(wL) \n",
        "  mbL = np.zeros_like(bL) \n",
        "  for k in range(0,L): # for each hidden layer k,\n",
        "    mW[k] = np.zeros_like(W[k])\n",
        "    mB[k] = np.zeros_like(B[k]) \n",
        "  #------------------------------------------------\n",
        "  ea_C = -np.log(1.0/vocab_size) # loss at iteration 0\n",
        "  print('Beginning C and perplexity: %f, %f' % (ea_C, np.exp(ea_C)))\n",
        "  \n",
        "  epoch=-1\n",
        "  for l in range(0,L):\n",
        "    last_h[l] = np.zeros((hlayer_size,1)) # reset memory\n",
        "    last_c[l] = np.zeros((hlayer_size,1)) # reset memory\n",
        "\n",
        "  while True:\n",
        "    # prepare X\n",
        "    if data_read_point+num_steps >= len(data) or num_of_steps == 0:\n",
        "      if epoch >= 0:\n",
        "        #==== Pyplot Epoch-End Plotting\n",
        "        plt.clf()\n",
        "        plt.axis([0,num_of_steps,0,vocab_size])\n",
        "        #plt.axis([0,num_of_steps,0,67])\n",
        "        #plt.yscale('log') #To active this, adjust y to 10^something or remove in axis setting\n",
        "        plt.plot(plot_x, plot_y1, label=\"Raw\")\n",
        "        plt.plot(plot_x, plot_y2, label=\"Exp Avg\")\n",
        "        plt.title(\"Perplexity\")\n",
        "        plt.xlabel('Steps')\n",
        "        plt.legend(loc='upper right')\n",
        "        plt.pause(0.0001)\n",
        "        #plt.show()\n",
        "        plt.draw()\n",
        "        #====================\n",
        "      epoch += 1\n",
        "      '''\n",
        "      for l in range(0,L):\n",
        "        last_h[l] = np.zeros((hlayer_size,1)) # reset memory\n",
        "        last_c[l] = np.zeros((hlayer_size,1)) # reset memory\n",
        "      '''\n",
        "      data_read_point = 0\n",
        "    try:\n",
        "      X = [word_to_idx[word] for word in data[data_read_point:data_read_point+num_steps]]\n",
        "      Y = [word_to_idx[word] for word in data[data_read_point+1:data_read_point+(num_steps+1)]]\n",
        "    except:\n",
        "      print(\"Data Read Error at data_read_point=%d and num_steps=%d\\n\" %(data_read_point, num_steps))\n",
        "      exit(0)\n",
        "    \n",
        "    #= One Step: Sequential feedforwarding of X one by one and one BPTT at the end of the step\n",
        "    sumOfallC, dwL, dbL, dW, dB = ForwardBackward(X, Y)\n",
        "    \n",
        "    #= Convergence Report\n",
        "    ea_C = ea_C * 0.999 + (sumOfallC/len(Y)) * 0.001\n",
        "    \n",
        "    #==== Pyplot Plotting Data\n",
        "    plot_x.append(num_of_steps-1)\n",
        "    plot_y1.append(np.exp(sumOfallC/len(Y)))\n",
        "    plot_y2.append(np.exp(ea_C))\n",
        "\n",
        "    if num_of_steps % 10 == 0 and num_of_steps > 0:\n",
        "      print('Epoch %d, Step %d: C, perplexity= %8.4f, %8.2f ==> Exp. Avg: %8.4f, %8.2f' % \n",
        "            (epoch, num_of_steps, sumOfallC/len(Y), np.exp(sumOfallC/len(Y)), ea_C, np.exp(ea_C)))\n",
        "    \n",
        "      #==== Pyplot Plotting\n",
        "      '''\n",
        "      plot_x.append(num_of_steps-1)\n",
        "      plot_y1.append(np.exp(sumOfallC/len(Y)))\n",
        "      plot_y2.append(np.exp(ea_C))\n",
        "      '''\n",
        "      if num_of_steps % 100 == 0:\n",
        "        plt.clf()\n",
        "        plt.axis([0,num_of_steps,0,vocab_size])\n",
        "        #plt.yscale('log') #To active this, adjust y to 10^something or remove in axis setting\n",
        "        plt.plot(plot_x, plot_y1, label=\"Raw\")\n",
        "        plt.plot(plot_x, plot_y2, label=\"Exp Avg\")\n",
        "        plt.title(\"Perplexity\")\n",
        "        plt.xlabel('Steps')\n",
        "        plt.legend(loc='upper right')\n",
        "        plt.pause(0.0001)\n",
        "        #plt.show()\n",
        "        plt.draw()\n",
        "      #====================\n",
        "\n",
        "    #=============== Creation =============\n",
        "    if num_of_steps % 100 == 0:\n",
        "      creation_idx = creation(Y[-1], 5)\n",
        "      if mode==\"c\":\n",
        "        created = \"Given: [...\"\n",
        "        created += ''.join(idx_to_word[idx] for idx in X[-2:-1])\n",
        "        created += (\"]\")\n",
        "        created += ''.join(idx_to_word[idx] for idx in creation_idx)\n",
        "      elif mode==\"w\":\n",
        "        created = \"Given: [... \"\n",
        "        created += ' '.join(idx_to_word[idx] for idx in X[-2:-1])\n",
        "        created += (\"] \")\n",
        "        created += ' '.join(idx_to_word[idx] for idx in creation_idx)\n",
        "      else:\n",
        "        print (\"mode value is incorrect\\n\")\n",
        "        exit(0)\n",
        "      print(\"\\n--My Mimicry--\")\n",
        "      print(\"%s\" % created)\n",
        "      print(\"---------------\\n\")\n",
        "    #========================================\n",
        "    \n",
        "    #================== Adjust w, u, and b ====================\n",
        "    for param, dparam, mem in zip([wL, bL],[dwL, dbL],[mwL, mbL]):\n",
        "      mem += dparam * dparam\n",
        "      param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
        "    \n",
        "    for param, dparam, mem in zip([W, B],[dW, dB],[mW, mB]):\n",
        "      for k in range(0,L):\n",
        "        mem[k] += dparam[k] * dparam[k]\n",
        "        param[k] += -learning_rate * dparam[k] / np.sqrt(mem[k] + 1e-8) # adagrad update\n",
        "    #===========================================================\n",
        "    \n",
        "    data_read_point += num_steps\n",
        "    #data_read_point += 1\n",
        "    num_of_steps += 1\n",
        "    \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SyVu3Ymhs77",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  '''\n",
        "  #=== Pyplot variables\n",
        "  plt.figure(\"deepSRN.py -- Byunggu Yu\")\n",
        "  #====================\n",
        "\n",
        "  #==== Pyplot Plotting\n",
        "  plt.clf()\n",
        "  #plt.axis([0,num_of_steps,0,vocab_size])\n",
        "  plt.yscale('log')\n",
        "  plt.plot(plot_x[0:40000], plot_y1[0:40000], label=\"Raw\")\n",
        "  plt.plot(plot_x[0:40000], plot_y2[0:40000], label=\"Exp Avg\")\n",
        "  plt.title(\"Perplexity\")\n",
        "  plt.xlabel('Steps')\n",
        "  plt.legend(loc='upper right')\n",
        "  #plt.pause(0.0001)\n",
        "  plt.show()\n",
        "  #plt.draw()\n",
        "  #====================\n",
        "  '''"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}